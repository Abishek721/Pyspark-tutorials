{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    krish| 31|        10| 30000|\n",
      "|sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 30000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import*\n",
    "spark = SparkSession.builder.appName('practise').getOrCreate()\n",
    "df_pyspark = spark.read.csv('py_details1.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-7P0V55Q:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x177d22c88d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    krish| 31|        10| 30000|\n",
      "|sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 30000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "|age|     name|salary|\n",
      "+---+---------+------+\n",
      "| 31|    krish| 30000|\n",
      "| 30|sudhanshu| 25000|\n",
      "| 29|    Sunny| 20000|\n",
      "| 24|     Paul| 30000|\n",
      "| 21|   Harsha| 15000|\n",
      "| 23|  Shubham| 18000|\n",
      "+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('age','name','salary').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    krish| 31|        10| 30000|\n",
      "|sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 30000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+------------------------+\n",
      "|     Name|Age|Experience|Salary|experience after two yrs|\n",
      "+---------+---+----------+------+------------------------+\n",
      "|    krish| 31|        10| 30000|                      12|\n",
      "|sudhanshu| 30|         8| 25000|                      10|\n",
      "|    Sunny| 29|         4| 20000|                       6|\n",
      "|     Paul| 24|         3| 30000|                       5|\n",
      "|   Harsha| 21|         1| 15000|                       3|\n",
      "|  Shubham| 23|         2| 18000|                       4|\n",
      "+---------+---+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('experience after two yrs',df_pyspark['experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|     Name|Experience|Salary|\n",
      "+---------+----------+------+\n",
      "|    krish|        10| 30000|\n",
      "|sudhanshu|         8| 25000|\n",
      "|    Sunny|         4| 20000|\n",
      "|     Paul|         3| 30000|\n",
      "|   Harsha|         1| 15000|\n",
      "|  Shubham|         2| 18000|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+------+\n",
      "|     Name|age_new|Experience|Salary|\n",
      "+---------+-------+----------+------+\n",
      "|    krish|     31|        10| 30000|\n",
      "|sudhanshu|     30|         8| 25000|\n",
      "|    Sunny|     29|         4| 20000|\n",
      "|     Paul|     24|         3| 30000|\n",
      "|   Harsha|     21|         1| 15000|\n",
      "|  Shubham|     23|         2| 18000|\n",
      "+---------+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##renaming the column\n",
    "df_pyspark.withColumnRenamed('age','age_new').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('practise2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|     krish| 34|        10| 30000|\n",
      "|sudharshan| 15|         8| 25000|\n",
      "|     suuny| 25|         4| 20000|\n",
      "|      paul| 58|         1| 15000|\n",
      "|    harsha| 55|         5| 18100|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark= spark.read.csv('py_details.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|     krish| 34|        10| 30000|\n",
      "|sudharshan| 15|         8| 25000|\n",
      "|     suuny| 25|         4| 20000|\n",
      "|      paul| 58|         1| 15000|\n",
      "|    harsha| 55|         5| 18100|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##how is any\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|     krish| 34|        10| 30000|\n",
      "|sudharshan| 15|         8| 25000|\n",
      "|     suuny| 25|         4| 20000|\n",
      "|      paul| 58|         1| 15000|\n",
      "|    harsha| 55|         5| 18100|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshlold\n",
    "df_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|     krish| 34|        10| 30000|\n",
      "|sudharshan| 15|         8| 25000|\n",
      "|     suuny| 25|         4| 20000|\n",
      "|      paul| 58|         1| 15000|\n",
      "|    harsha| 55|         5| 18100|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##subset\n",
    "df_pyspark.na.drop(how='any',subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|     krish| 34|        10| 30000|\n",
      "|sudharshan| 15|         8| 25000|\n",
      "|     suuny| 25|         4| 20000|\n",
      "|      paul| 58|         1| 15000|\n",
      "|    harsha| 55|         5| 18100|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('missing values','salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|age|experience|salary|\n",
      "+----------+---+----------+------+\n",
      "|     krish| 34|        10| 30000|\n",
      "|sudharshan| 15|         8| 25000|\n",
      "|     suuny| 25|         4| 20000|\n",
      "|      paul| 58|         1| 15000|\n",
      "|    harsha| 55|         5| 18100|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "Imputer=Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=['{}_imputed'.format(c) for c in ['age','Experience','Salary']]\n",
    ").setStrategy('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      name|age|\n",
      "+----------+---+\n",
      "|     krish| 34|\n",
      "|sudharshan| 15|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##filter operation\n",
    "df_pyspark.filter(~(df_pyspark[\"salary\"]<=20000)).select('name','age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('practise2').getOrCreate()\n",
    "df_pyspark= spark.read.csv('py_details1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    krish| 31|        10| 30000|\n",
      "|sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 30000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'departments' does not exist. Did you mean one of the following? [Experience, Name, Salary, Age];\n'Aggregate ['departments], ['departments, avg(Age#1291) AS avg(Age)#1327, avg(Experience#1292) AS avg(Experience)#1328, avg(Salary#1293) AS avg(Salary)#1329]\n+- Relation [Name#1290,Age#1291,Experience#1292,Salary#1293] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m##groupby function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m## grouped to find maximum salary \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_pyspark\u001b[39m.\u001b[39;49mgroupBy(\u001b[39m\"\u001b[39;49m\u001b[39mdepartments\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmean()\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\group.py:50\u001b[0m, in \u001b[0;36mdf_varargs_api.<locals>._api\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_api\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mGroupedData\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mcols: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m     49\u001b[0m     name \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m---> 50\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jgd, name)(_to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49m_sc, cols))\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column 'departments' does not exist. Did you mean one of the following? [Experience, Name, Salary, Age];\n'Aggregate ['departments], ['departments, avg(Age#1291) AS avg(Age)#1327, avg(Experience#1292) AS avg(Experience)#1328, avg(Salary#1293) AS avg(Salary)#1329]\n+- Relation [Name#1290,Age#1291,Experience#1292,Salary#1293] csv\n"
     ]
    }
   ],
   "source": [
    "##groupby function\n",
    "## grouped to find maximum salary \n",
    "df_pyspark.groupBy(\"departments\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|departments|min(salary)|\n",
      "+-----------+-----------+\n",
      "|        IOT|       5000|\n",
      "|    BigData|       2000|\n",
      "|datascience|       3000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## totla employess working\n",
    "df_pyspark.groupBy(\"departments\").min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(name)|\n",
      "+-----------+\n",
      "|         10|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({\"name\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##mllib exmaples explianed\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('mlib').getOrCreate()\n",
    "training =spark.read.csv('py_details1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    krish| 31|        10| 30000|\n",
      "|sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 30000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=['Age','Experience'],outputCol='independentfeatures')\n",
    "output= featureassembler.transform(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+-------------------+\n",
      "|     Name|Age|Experience|Salary|independentfeatures|\n",
      "+---------+---+----------+------+-------------------+\n",
      "|    krish| 31|        10| 30000|        [31.0,10.0]|\n",
      "|sudhanshu| 30|         8| 25000|         [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|         [29.0,4.0]|\n",
      "|     Paul| 24|         3| 30000|         [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|         [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|         [23.0,2.0]|\n",
      "+---------+---+----------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary', 'independentfeatures']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.show()\n",
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|independentfeatures|salary|\n",
      "+-------------------+------+\n",
      "|        [31.0,10.0]| 30000|\n",
      "|         [30.0,8.0]| 25000|\n",
      "|         [29.0,4.0]| 20000|\n",
      "|         [24.0,3.0]| 30000|\n",
      "|         [21.0,1.0]| 15000|\n",
      "|         [23.0,2.0]| 18000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data= output.select('independentfeatures','salary')\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data,test_data= finalized_data.randomSplit([0.75,0.25])\n",
    "regressor= LinearRegression(featuresCol='independentfeatures',labelCol='salary')\n",
    "regressor= regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-2537.5626, 2395.6594])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30611.51079136589"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+\n",
      "|independentfeatures|salary|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|         [23.0,2.0]| 18000|20000.000000000015|\n",
      "|         [30.0,8.0]| 25000| 74999.99999999987|\n",
      "|        [31.0,10.0]| 30000| 99999.99999999981|\n",
      "+-------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results= regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('abi').getOrCreate()\n",
    "abi_pyspark = spark.read.csv('tips.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: int]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abi_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "| sex| tip|smoker|\n",
      "+----+----+------+\n",
      "|Male|1.66|    No|\n",
      "|Male| 3.5|    No|\n",
      "|Male|3.31|    No|\n",
      "|Male|4.71|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male|3.12|    No|\n",
      "|Male|1.96|    No|\n",
      "|Male|3.23|    No|\n",
      "|Male|1.71|    No|\n",
      "|Male|1.57|    No|\n",
      "|Male| 3.0|    No|\n",
      "|Male|3.92|    No|\n",
      "|Male|3.71|    No|\n",
      "|Male|3.35|    No|\n",
      "|Male|4.08|    No|\n",
      "|Male|7.58|    No|\n",
      "|Male|3.18|    No|\n",
      "|Male|2.34|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male| 4.3|    No|\n",
      "|Male|1.45|    No|\n",
      "|Male| 2.5|    No|\n",
      "|Male|3.27|    No|\n",
      "|Male| 3.6|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male|2.31|    No|\n",
      "|Male| 5.0|    No|\n",
      "|Male|2.24|    No|\n",
      "|Male|2.54|    No|\n",
      "|Male|3.06|    No|\n",
      "|Male|1.32|    No|\n",
      "|Male| 5.6|    No|\n",
      "|Male| 3.0|    No|\n",
      "|Male| 5.0|    No|\n",
      "|Male| 6.0|    No|\n",
      "|Male|2.05|    No|\n",
      "|Male| 3.0|    No|\n",
      "|Male| 2.5|    No|\n",
      "|Male|1.56|    No|\n",
      "|Male|4.34|    No|\n",
      "|Male|3.51|    No|\n",
      "|Male| 3.0|   Yes|\n",
      "|Male|1.76|   Yes|\n",
      "|Male|6.73|    No|\n",
      "|Male|3.21|   Yes|\n",
      "|Male| 2.0|   Yes|\n",
      "|Male|1.98|   Yes|\n",
      "|Male|3.76|   Yes|\n",
      "|Male|2.64|    No|\n",
      "|Male|3.15|    No|\n",
      "|Male|2.01|    No|\n",
      "|Male|2.09|   Yes|\n",
      "|Male|1.97|    No|\n",
      "|Male|1.25|    No|\n",
      "|Male|3.08|   Yes|\n",
      "|Male| 4.0|    No|\n",
      "|Male| 3.0|    No|\n",
      "|Male|2.71|    No|\n",
      "|Male| 3.0|   Yes|\n",
      "|Male| 3.4|    No|\n",
      "|Male| 5.0|   Yes|\n",
      "|Male|2.03|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male| 4.0|    No|\n",
      "|Male|5.85|    No|\n",
      "|Male| 3.0|    No|\n",
      "|Male| 3.0|   Yes|\n",
      "|Male| 3.5|    No|\n",
      "|Male|4.73|   Yes|\n",
      "|Male| 4.0|   Yes|\n",
      "|Male| 1.5|   Yes|\n",
      "|Male| 3.0|   Yes|\n",
      "|Male| 1.5|    No|\n",
      "|Male|1.64|   Yes|\n",
      "|Male|4.06|   Yes|\n",
      "|Male|4.29|   Yes|\n",
      "|Male|3.76|    No|\n",
      "|Male| 3.0|    No|\n",
      "|Male| 4.0|    No|\n",
      "|Male|2.55|    No|\n",
      "|Male|5.07|    No|\n",
      "|Male|2.31|    No|\n",
      "|Male| 2.5|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male|1.48|    No|\n",
      "|Male|2.18|    No|\n",
      "|Male| 1.5|    No|\n",
      "|Male| 2.0|   Yes|\n",
      "|Male| 6.7|    No|\n",
      "|Male| 5.0|    No|\n",
      "|Male|1.73|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male| 2.5|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male|2.74|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male| 2.0|    No|\n",
      "|Male| 5.0|    No|\n",
      "|Male| 2.0|    No|\n",
      "+----+----+------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abi_pyspark.orderBy(desc('sex')).select('sex','tip','smoker').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de47f5c92c0ee6f12a59a5613ac5feff6aab19ddff207ba0b3964cced08c4ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
